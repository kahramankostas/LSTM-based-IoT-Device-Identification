{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dropout,Activation,Embedding\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "import time\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pprint\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "import talos\n",
    "from talos.model.normalizers import lr_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.772055</td>\n",
       "      <td>Aria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.772055</td>\n",
       "      <td>Aria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Aria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.772055</td>\n",
       "      <td>Aria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.772055</td>\n",
       "      <td>Aria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  291  292  293  294  \\\n",
       "0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  4.0  0.0   \n",
       "1  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  4.0  0.0   \n",
       "2  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  4.0  0.0   \n",
       "3  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  4.0  0.0   \n",
       "4  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  4.0  0.0   \n",
       "\n",
       "   295  296   297  298       299   300  \n",
       "0  3.0  1.0  64.0  1.0  3.772055  Aria  \n",
       "1  3.0  1.0  64.0  1.0  3.772055  Aria  \n",
       "2  1.0  3.0  40.0  0.0  0.000000  Aria  \n",
       "3  3.0  1.0  64.0  1.0  3.772055  Aria  \n",
       "4  3.0  1.0  64.0  1.0  3.772055  Aria  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('l_h_train_sm.csv',header=None )\n",
    "np.random.seed(42)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8100, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X =df[df.columns[0:-1]]\n",
    "X=np.array(X)\n",
    "\n",
    "y=df[df.columns[-1]]  \n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "X = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "le = preprocessing.LabelEncoder()\n",
    "y=le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 25, 25, 25])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test validation split - 60:20:20\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "Y_train = np.reshape(Y_train, (Y_train.shape[0],  Y_train.shape[1]))\n",
    "Y_val = np.reshape(Y_val, (Y_val.shape[0],  Y_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomInit = initializers.RandomUniform(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_best(x_train, y_train, x_val, y_val, params):\n",
    "\n",
    "##### NEW\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(params['first_neuron'], kernel_initializer = randomInit, bias_initializer = randomInit,\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    \n",
    "    #talos.utils.hidden_layers(model, params, 3)\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    #model.add(Embedding(input_dim=301, output_dim=27)) # 100000 is the vocab size, 128 is arbitrary\n",
    "    model.add(Dense(Y_train.shape[1], activation=params['last_activation']))\n",
    "\n",
    "    #nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "\n",
    "    model.compile(loss=params['losses'],\n",
    "                  optimizer=params['optimizer'],#(lr=lr_normalizer(params['lr'], params['optimizer']),\n",
    "                  metrics=['acc', talos.utils.metrics.f1score])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        callbacks=[talos.utils.live()],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can go ahead and set the parameter space\n",
    "p = {'first_neuron':[16,32,64,128],\n",
    "     'hidden_layers':[0, 1, 2,3, 4,5,6],\n",
    "     'batch_size': [5,10,25,50],\n",
    "     'epochs': [10,20,30],\n",
    "     'dropout': (0, 0.40,2, 4,10),\n",
    "     'lr':[0.1,0.01,0.001],\n",
    "     #'kernel_initializer': ['uniform','normal'],\n",
    "     'optimizer': ['Nadam', 'Adam',\"Adamax\",\"Adadelta\",\"Adagrad\",\"RMSprop\",\"SGD\"],\n",
    "     'losses': ['categorical_crossentropy'],\n",
    "     'activation':['relu', 'elu', \"softmax\" , \"selu\", \"softplus\", \"softsign\", \"tanh\", \"sigmoid\", \"hard_sigmoid\", \"exponential\", \"linear\"],\n",
    "     'last_activation': ['softmax']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [1:09:04<00:00, 41.44s/it]\n"
     ]
    }
   ],
   "source": [
    "# and run the experiment\n",
    "t = talos.Scan(x=X_train,\n",
    "               y=Y_train,\n",
    "               model=find_the_best,\n",
    "               params=p,\n",
    "               experiment_name='find_the_best',\n",
    "               round_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_f1score</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1score</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.919193</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.618227</td>\n",
       "      <td>0.952065</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.614157</td>\n",
       "      <td>softsign</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.298065</td>\n",
       "      <td>0.873971</td>\n",
       "      <td>0.863314</td>\n",
       "      <td>0.369382</td>\n",
       "      <td>0.860670</td>\n",
       "      <td>0.847572</td>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1.171706</td>\n",
       "      <td>0.724280</td>\n",
       "      <td>0.393703</td>\n",
       "      <td>1.302224</td>\n",
       "      <td>0.649912</td>\n",
       "      <td>0.372585</td>\n",
       "      <td>selu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1.547021</td>\n",
       "      <td>0.610597</td>\n",
       "      <td>0.129046</td>\n",
       "      <td>1.574374</td>\n",
       "      <td>0.613757</td>\n",
       "      <td>0.127748</td>\n",
       "      <td>elu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>0.201450</td>\n",
       "      <td>0.903292</td>\n",
       "      <td>0.901335</td>\n",
       "      <td>0.232921</td>\n",
       "      <td>0.891975</td>\n",
       "      <td>0.889241</td>\n",
       "      <td>softplus</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>30</td>\n",
       "      <td>0.433377</td>\n",
       "      <td>0.843107</td>\n",
       "      <td>0.823682</td>\n",
       "      <td>0.457308</td>\n",
       "      <td>0.834656</td>\n",
       "      <td>0.822573</td>\n",
       "      <td>softplus</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>20</td>\n",
       "      <td>0.317020</td>\n",
       "      <td>0.867798</td>\n",
       "      <td>0.866512</td>\n",
       "      <td>0.333124</td>\n",
       "      <td>0.876323</td>\n",
       "      <td>0.867286</td>\n",
       "      <td>relu</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>10</td>\n",
       "      <td>3.213871</td>\n",
       "      <td>0.253601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.217178</td>\n",
       "      <td>0.254850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>30</td>\n",
       "      <td>0.460231</td>\n",
       "      <td>0.835905</td>\n",
       "      <td>0.810857</td>\n",
       "      <td>0.545109</td>\n",
       "      <td>0.811287</td>\n",
       "      <td>0.791109</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "      <td>1.307673</td>\n",
       "      <td>0.656893</td>\n",
       "      <td>0.282641</td>\n",
       "      <td>1.441581</td>\n",
       "      <td>0.618386</td>\n",
       "      <td>0.243559</td>\n",
       "      <td>softmax</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs  val_loss   val_acc  val_f1score      loss       acc  \\\n",
       "0             30  0.919193  0.750000     0.618227  0.952065  0.740741   \n",
       "1             30  0.298065  0.873971     0.863314  0.369382  0.860670   \n",
       "2             20  1.171706  0.724280     0.393703  1.302224  0.649912   \n",
       "3             10  1.547021  0.610597     0.129046  1.574374  0.613757   \n",
       "4             30  0.201450  0.903292     0.901335  0.232921  0.891975   \n",
       "..           ...       ...       ...          ...       ...       ...   \n",
       "95            30  0.433377  0.843107     0.823682  0.457308  0.834656   \n",
       "96            20  0.317020  0.867798     0.866512  0.333124  0.876323   \n",
       "97            10  3.213871  0.253601     0.000000  3.217178  0.254850   \n",
       "98            30  0.460231  0.835905     0.810857  0.545109  0.811287   \n",
       "99            10  1.307673  0.656893     0.282641  1.441581  0.618386   \n",
       "\n",
       "     f1score    activation  batch_size  dropout  epochs  first_neuron  \\\n",
       "0   0.614157      softsign          50      0.0      30            16   \n",
       "1   0.847572        linear          10      0.2      30            32   \n",
       "2   0.372585          selu          50      0.2      20            16   \n",
       "3   0.127748           elu           5      0.0      10            16   \n",
       "4   0.889241      softplus           5      0.2      30            64   \n",
       "..       ...           ...         ...      ...     ...           ...   \n",
       "95  0.822573      softplus          10      0.0      30            32   \n",
       "96  0.867286          relu          25      0.0      20           128   \n",
       "97  0.000000          relu          10      0.2      10            64   \n",
       "98  0.791109  hard_sigmoid          10      0.2      30            32   \n",
       "99  0.243559       softmax          10      0.2      10            16   \n",
       "\n",
       "    hidden_layers last_activation                    losses     lr optimizer  \n",
       "0               5         softmax  categorical_crossentropy  0.010    Adamax  \n",
       "1               3         softmax  categorical_crossentropy  0.100      Adam  \n",
       "2               0         softmax  categorical_crossentropy  0.010   RMSprop  \n",
       "3               5         softmax  categorical_crossentropy  0.001   Adagrad  \n",
       "4               3         softmax  categorical_crossentropy  0.001      Adam  \n",
       "..            ...             ...                       ...    ...       ...  \n",
       "95              5         softmax  categorical_crossentropy  0.001    Adamax  \n",
       "96              0         softmax  categorical_crossentropy  0.010      Adam  \n",
       "97              1         softmax  categorical_crossentropy  0.100       SGD  \n",
       "98              3         softmax  categorical_crossentropy  0.100  Adadelta  \n",
       "99              3         softmax  categorical_crossentropy  0.010    Adamax  \n",
       "\n",
       "[100 rows x 17 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_f1score</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1score</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.919193</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.618227</td>\n",
       "      <td>0.952065</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.614157</td>\n",
       "      <td>softsign</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.298065</td>\n",
       "      <td>0.873971</td>\n",
       "      <td>0.863314</td>\n",
       "      <td>0.369382</td>\n",
       "      <td>0.860670</td>\n",
       "      <td>0.847572</td>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1.171706</td>\n",
       "      <td>0.724280</td>\n",
       "      <td>0.393703</td>\n",
       "      <td>1.302224</td>\n",
       "      <td>0.649912</td>\n",
       "      <td>0.372585</td>\n",
       "      <td>selu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1.547021</td>\n",
       "      <td>0.610597</td>\n",
       "      <td>0.129046</td>\n",
       "      <td>1.574374</td>\n",
       "      <td>0.613757</td>\n",
       "      <td>0.127748</td>\n",
       "      <td>elu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>0.201450</td>\n",
       "      <td>0.903292</td>\n",
       "      <td>0.901335</td>\n",
       "      <td>0.232921</td>\n",
       "      <td>0.891975</td>\n",
       "      <td>0.889241</td>\n",
       "      <td>softplus</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.388188</td>\n",
       "      <td>0.852366</td>\n",
       "      <td>0.849394</td>\n",
       "      <td>0.436741</td>\n",
       "      <td>0.839727</td>\n",
       "      <td>0.831194</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>3.252050</td>\n",
       "      <td>0.212449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.250037</td>\n",
       "      <td>0.181437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.787146</td>\n",
       "      <td>0.780864</td>\n",
       "      <td>0.679515</td>\n",
       "      <td>0.892992</td>\n",
       "      <td>0.742725</td>\n",
       "      <td>0.643693</td>\n",
       "      <td>softplus</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.841300</td>\n",
       "      <td>0.783436</td>\n",
       "      <td>0.677695</td>\n",
       "      <td>0.901977</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.647758</td>\n",
       "      <td>elu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>0.176669</td>\n",
       "      <td>0.905350</td>\n",
       "      <td>0.904207</td>\n",
       "      <td>0.195699</td>\n",
       "      <td>0.902998</td>\n",
       "      <td>0.902986</td>\n",
       "      <td>elu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.306909</td>\n",
       "      <td>0.875514</td>\n",
       "      <td>0.872594</td>\n",
       "      <td>0.325474</td>\n",
       "      <td>0.881173</td>\n",
       "      <td>0.873232</td>\n",
       "      <td>elu</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>0.900664</td>\n",
       "      <td>0.744342</td>\n",
       "      <td>0.664503</td>\n",
       "      <td>1.048586</td>\n",
       "      <td>0.704365</td>\n",
       "      <td>0.583959</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0.465570</td>\n",
       "      <td>0.834362</td>\n",
       "      <td>0.803614</td>\n",
       "      <td>0.477175</td>\n",
       "      <td>0.831349</td>\n",
       "      <td>0.810713</td>\n",
       "      <td>softmax</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>0.529306</td>\n",
       "      <td>0.818930</td>\n",
       "      <td>0.789273</td>\n",
       "      <td>0.556243</td>\n",
       "      <td>0.813492</td>\n",
       "      <td>0.783395</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0.212554</td>\n",
       "      <td>0.901235</td>\n",
       "      <td>0.902020</td>\n",
       "      <td>0.221194</td>\n",
       "      <td>0.900573</td>\n",
       "      <td>0.898344</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>0.456706</td>\n",
       "      <td>0.834877</td>\n",
       "      <td>0.824819</td>\n",
       "      <td>0.521018</td>\n",
       "      <td>0.813051</td>\n",
       "      <td>0.799873</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.359911</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.843868</td>\n",
       "      <td>0.387775</td>\n",
       "      <td>0.862434</td>\n",
       "      <td>0.839518</td>\n",
       "      <td>elu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>0.527339</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>0.805474</td>\n",
       "      <td>0.634420</td>\n",
       "      <td>0.789462</td>\n",
       "      <td>0.759148</td>\n",
       "      <td>exponential</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>0.427277</td>\n",
       "      <td>0.843621</td>\n",
       "      <td>0.827025</td>\n",
       "      <td>0.444218</td>\n",
       "      <td>0.839727</td>\n",
       "      <td>0.828928</td>\n",
       "      <td>softsign</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>0.179791</td>\n",
       "      <td>0.909465</td>\n",
       "      <td>0.908707</td>\n",
       "      <td>0.175771</td>\n",
       "      <td>0.917548</td>\n",
       "      <td>0.914826</td>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3.231671</td>\n",
       "      <td>0.076132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.232169</td>\n",
       "      <td>0.113316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>softsign</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>3.281244</td>\n",
       "      <td>0.104938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.280578</td>\n",
       "      <td>0.113095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>softmax</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>0.331475</td>\n",
       "      <td>0.868313</td>\n",
       "      <td>0.863251</td>\n",
       "      <td>0.336762</td>\n",
       "      <td>0.868166</td>\n",
       "      <td>0.859995</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>0.546739</td>\n",
       "      <td>0.826646</td>\n",
       "      <td>0.779101</td>\n",
       "      <td>0.581149</td>\n",
       "      <td>0.811508</td>\n",
       "      <td>0.768388</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>0.987264</td>\n",
       "      <td>0.742798</td>\n",
       "      <td>0.599252</td>\n",
       "      <td>1.052141</td>\n",
       "      <td>0.724868</td>\n",
       "      <td>0.577931</td>\n",
       "      <td>elu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>3.238771</td>\n",
       "      <td>0.240226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.236110</td>\n",
       "      <td>0.243166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>0.248282</td>\n",
       "      <td>0.896091</td>\n",
       "      <td>0.885473</td>\n",
       "      <td>0.253151</td>\n",
       "      <td>0.902116</td>\n",
       "      <td>0.896191</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>0.606398</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>0.742964</td>\n",
       "      <td>0.755634</td>\n",
       "      <td>0.755952</td>\n",
       "      <td>0.686720</td>\n",
       "      <td>selu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>0.317776</td>\n",
       "      <td>0.865226</td>\n",
       "      <td>0.864871</td>\n",
       "      <td>0.370343</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.855081</td>\n",
       "      <td>softmax</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>0.198956</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.904556</td>\n",
       "      <td>0.252181</td>\n",
       "      <td>0.887566</td>\n",
       "      <td>0.884421</td>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1.407274</td>\n",
       "      <td>0.660494</td>\n",
       "      <td>0.200068</td>\n",
       "      <td>1.507228</td>\n",
       "      <td>0.611993</td>\n",
       "      <td>0.197381</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>0.604499</td>\n",
       "      <td>0.818930</td>\n",
       "      <td>0.772426</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.766534</td>\n",
       "      <td>0.710387</td>\n",
       "      <td>linear</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>0.476760</td>\n",
       "      <td>0.841049</td>\n",
       "      <td>0.804296</td>\n",
       "      <td>0.503422</td>\n",
       "      <td>0.832011</td>\n",
       "      <td>0.806227</td>\n",
       "      <td>softplus</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>3.204770</td>\n",
       "      <td>0.220679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.208324</td>\n",
       "      <td>0.228175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>elu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>0.593843</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.775208</td>\n",
       "      <td>0.687377</td>\n",
       "      <td>0.775132</td>\n",
       "      <td>0.738857</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>0.172531</td>\n",
       "      <td>0.912037</td>\n",
       "      <td>0.910094</td>\n",
       "      <td>0.166897</td>\n",
       "      <td>0.910935</td>\n",
       "      <td>0.910874</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>0.841345</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.670481</td>\n",
       "      <td>0.903800</td>\n",
       "      <td>0.749559</td>\n",
       "      <td>0.668872</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>0.605598</td>\n",
       "      <td>0.801440</td>\n",
       "      <td>0.764689</td>\n",
       "      <td>0.633503</td>\n",
       "      <td>0.787478</td>\n",
       "      <td>0.761566</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>0.702125</td>\n",
       "      <td>0.784979</td>\n",
       "      <td>0.714437</td>\n",
       "      <td>0.738753</td>\n",
       "      <td>0.780864</td>\n",
       "      <td>0.711900</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>0.468865</td>\n",
       "      <td>0.829733</td>\n",
       "      <td>0.825966</td>\n",
       "      <td>0.528728</td>\n",
       "      <td>0.814374</td>\n",
       "      <td>0.798334</td>\n",
       "      <td>softmax</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>0.437709</td>\n",
       "      <td>0.845165</td>\n",
       "      <td>0.826340</td>\n",
       "      <td>0.498447</td>\n",
       "      <td>0.820326</td>\n",
       "      <td>0.811072</td>\n",
       "      <td>softsign</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>1.459488</td>\n",
       "      <td>0.636831</td>\n",
       "      <td>0.248961</td>\n",
       "      <td>1.558978</td>\n",
       "      <td>0.599647</td>\n",
       "      <td>0.187951</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>0.164038</td>\n",
       "      <td>0.903807</td>\n",
       "      <td>0.903292</td>\n",
       "      <td>0.160802</td>\n",
       "      <td>0.920855</td>\n",
       "      <td>0.919683</td>\n",
       "      <td>elu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>3.195099</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.198715</td>\n",
       "      <td>0.264991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>0.206639</td>\n",
       "      <td>0.903807</td>\n",
       "      <td>0.900419</td>\n",
       "      <td>0.234198</td>\n",
       "      <td>0.896605</td>\n",
       "      <td>0.893815</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>0.922650</td>\n",
       "      <td>0.758230</td>\n",
       "      <td>0.584174</td>\n",
       "      <td>0.982811</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.557632</td>\n",
       "      <td>elu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>20</td>\n",
       "      <td>0.922224</td>\n",
       "      <td>0.753601</td>\n",
       "      <td>0.619760</td>\n",
       "      <td>0.953460</td>\n",
       "      <td>0.738977</td>\n",
       "      <td>0.606785</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>3.283136</td>\n",
       "      <td>0.055041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.282167</td>\n",
       "      <td>0.065256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>30</td>\n",
       "      <td>0.217853</td>\n",
       "      <td>0.899177</td>\n",
       "      <td>0.897498</td>\n",
       "      <td>0.223422</td>\n",
       "      <td>0.902998</td>\n",
       "      <td>0.902950</td>\n",
       "      <td>linear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.804012</td>\n",
       "      <td>0.759165</td>\n",
       "      <td>0.661264</td>\n",
       "      <td>0.788801</td>\n",
       "      <td>0.747915</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  round_epochs  val_loss   val_acc  val_f1score      loss  \\\n",
       "0            0            30  0.919193  0.750000     0.618227  0.952065   \n",
       "1            1            30  0.298065  0.873971     0.863314  0.369382   \n",
       "2            2            20  1.171706  0.724280     0.393703  1.302224   \n",
       "3            3            10  1.547021  0.610597     0.129046  1.574374   \n",
       "4            4            30  0.201450  0.903292     0.901335  0.232921   \n",
       "5            5            20  0.388188  0.852366     0.849394  0.436741   \n",
       "6            6            20  3.252050  0.212449     0.000000  3.250037   \n",
       "7            7            10  0.787146  0.780864     0.679515  0.892992   \n",
       "8            8            10  0.841300  0.783436     0.677695  0.901977   \n",
       "9            9            30  0.176669  0.905350     0.904207  0.195699   \n",
       "10          10            20  0.306909  0.875514     0.872594  0.325474   \n",
       "11          11            20  0.900664  0.744342     0.664503  1.048586   \n",
       "12          12            20  0.465570  0.834362     0.803614  0.477175   \n",
       "13          13            30  0.529306  0.818930     0.789273  0.556243   \n",
       "14          14            10  0.212554  0.901235     0.902020  0.221194   \n",
       "15          15            10  0.456706  0.834877     0.824819  0.521018   \n",
       "16          16            10  0.359911  0.861111     0.843868  0.387775   \n",
       "17          17            10  0.527339  0.816872     0.805474  0.634420   \n",
       "18          18            20  0.427277  0.843621     0.827025  0.444218   \n",
       "19          19            30  0.179791  0.909465     0.908707  0.175771   \n",
       "20          20            10  3.231671  0.076132     0.000000  3.232169   \n",
       "21          21            10  3.281244  0.104938     0.000000  3.280578   \n",
       "22          22            30  0.331475  0.868313     0.863251  0.336762   \n",
       "23          23            20  0.546739  0.826646     0.779101  0.581149   \n",
       "24          24            10  0.987264  0.742798     0.599252  1.052141   \n",
       "25          25            30  3.238771  0.240226     0.000000  3.236110   \n",
       "26          26            30  0.248282  0.896091     0.885473  0.253151   \n",
       "27          27            30  0.606398  0.816872     0.742964  0.755634   \n",
       "28          28            30  0.317776  0.865226     0.864871  0.370343   \n",
       "29          29            30  0.198956  0.902778     0.904556  0.252181   \n",
       "30          30            30  1.407274  0.660494     0.200068  1.507228   \n",
       "31          31            20  0.604499  0.818930     0.772426  0.766000   \n",
       "32          32            20  0.476760  0.841049     0.804296  0.503422   \n",
       "33          33            10  3.204770  0.220679     0.000000  3.208324   \n",
       "34          34            20  0.593843  0.802469     0.775208  0.687377   \n",
       "35          35            30  0.172531  0.912037     0.910094  0.166897   \n",
       "36          36            10  0.841345  0.768519     0.670481  0.903800   \n",
       "37          37            20  0.605598  0.801440     0.764689  0.633503   \n",
       "38          38            10  0.702125  0.784979     0.714437  0.738753   \n",
       "39          39            20  0.468865  0.829733     0.825966  0.528728   \n",
       "40          40            30  0.437709  0.845165     0.826340  0.498447   \n",
       "41          41            10  1.459488  0.636831     0.248961  1.558978   \n",
       "42          42            30  0.164038  0.903807     0.903292  0.160802   \n",
       "43          43            10  3.195099  0.259259     0.000000  3.198715   \n",
       "44          44            30  0.206639  0.903807     0.900419  0.234198   \n",
       "45          45            10  0.922650  0.758230     0.584174  0.982811   \n",
       "46          46            20  0.922224  0.753601     0.619760  0.953460   \n",
       "47          47            10  3.283136  0.055041     0.000000  3.282167   \n",
       "48          48            30  0.217853  0.899177     0.897498  0.223422   \n",
       "49          49            10  0.614148  0.804012     0.759165  0.661264   \n",
       "\n",
       "         acc   f1score    activation  batch_size  dropout  epochs  \\\n",
       "0   0.740741  0.614157      softsign          50      0.0      30   \n",
       "1   0.860670  0.847572        linear          10      0.2      30   \n",
       "2   0.649912  0.372585          selu          50      0.2      20   \n",
       "3   0.613757  0.127748           elu           5      0.0      10   \n",
       "4   0.891975  0.889241      softplus           5      0.2      30   \n",
       "5   0.839727  0.831194   exponential           5      0.2      20   \n",
       "6   0.181437  0.000000          selu          25      0.2      20   \n",
       "7   0.742725  0.643693      softplus          25      0.2      10   \n",
       "8   0.767857  0.647758           elu          50      0.0      10   \n",
       "9   0.902998  0.902986           elu           5      0.2      30   \n",
       "10  0.881173  0.873232           elu          25      0.0      20   \n",
       "11  0.704365  0.583959       sigmoid          50      0.2      20   \n",
       "12  0.831349  0.810713       softmax           5      0.0      20   \n",
       "13  0.813492  0.783395  hard_sigmoid          50      0.0      30   \n",
       "14  0.900573  0.898344       sigmoid           5      0.0      10   \n",
       "15  0.813051  0.799873          tanh           5      0.2      10   \n",
       "16  0.862434  0.839518           elu           5      0.0      10   \n",
       "17  0.789462  0.759148   exponential          25      0.2      10   \n",
       "18  0.839727  0.828928      softsign          10      0.0      20   \n",
       "19  0.917548  0.914826        linear          10      0.0      30   \n",
       "20  0.113316  0.000000      softsign          10      0.2      10   \n",
       "21  0.113095  0.000000       softmax          50      0.2      10   \n",
       "22  0.868166  0.859995          tanh          10      0.0      30   \n",
       "23  0.811508  0.768388          relu          10      0.0      20   \n",
       "24  0.724868  0.577931           elu          50      0.0      10   \n",
       "25  0.243166  0.000000  hard_sigmoid          50      0.2      30   \n",
       "26  0.902116  0.896191          relu          10      0.0      30   \n",
       "27  0.755952  0.686720          selu           5      0.2      30   \n",
       "28  0.861111  0.855081       softmax          10      0.2      30   \n",
       "29  0.887566  0.884421        linear          10      0.2      30   \n",
       "30  0.611993  0.197381   exponential          50      0.2      30   \n",
       "31  0.766534  0.710387        linear          50      0.2      20   \n",
       "32  0.832011  0.806227      softplus           5      0.0      20   \n",
       "33  0.228175  0.000000           elu          10      0.0      10   \n",
       "34  0.775132  0.738857  hard_sigmoid          25      0.2      20   \n",
       "35  0.910935  0.910874       sigmoid           5      0.0      30   \n",
       "36  0.749559  0.668872   exponential          50      0.0      10   \n",
       "37  0.787478  0.761566       sigmoid          10      0.0      20   \n",
       "38  0.780864  0.711900  hard_sigmoid          10      0.0      10   \n",
       "39  0.814374  0.798334       softmax          25      0.2      20   \n",
       "40  0.820326  0.811072      softsign          25      0.2      30   \n",
       "41  0.599647  0.187951          tanh          50      0.2      10   \n",
       "42  0.920855  0.919683           elu          10      0.0      30   \n",
       "43  0.264991  0.000000   exponential          10      0.0      10   \n",
       "44  0.896605  0.893815  hard_sigmoid          10      0.2      30   \n",
       "45  0.740741  0.557632           elu           5      0.0      10   \n",
       "46  0.738977  0.606785  hard_sigmoid           5      0.0      20   \n",
       "47  0.065256  0.000000          tanh          50      0.0      10   \n",
       "48  0.902998  0.902950        linear          10      0.0      30   \n",
       "49  0.788801  0.747915          tanh          50      0.0      10   \n",
       "\n",
       "    first_neuron  hidden_layers last_activation                    losses  \\\n",
       "0             16              5         softmax  categorical_crossentropy   \n",
       "1             32              3         softmax  categorical_crossentropy   \n",
       "2             16              0         softmax  categorical_crossentropy   \n",
       "3             16              5         softmax  categorical_crossentropy   \n",
       "4             64              3         softmax  categorical_crossentropy   \n",
       "5             64              3         softmax  categorical_crossentropy   \n",
       "6             32              6         softmax  categorical_crossentropy   \n",
       "7             32              5         softmax  categorical_crossentropy   \n",
       "8             16              5         softmax  categorical_crossentropy   \n",
       "9            128              2         softmax  categorical_crossentropy   \n",
       "10            32              3         softmax  categorical_crossentropy   \n",
       "11            16              3         softmax  categorical_crossentropy   \n",
       "12            32              2         softmax  categorical_crossentropy   \n",
       "13            32              6         softmax  categorical_crossentropy   \n",
       "14           128              6         softmax  categorical_crossentropy   \n",
       "15           128              6         softmax  categorical_crossentropy   \n",
       "16            16              0         softmax  categorical_crossentropy   \n",
       "17            32              2         softmax  categorical_crossentropy   \n",
       "18            32              1         softmax  categorical_crossentropy   \n",
       "19            32              2         softmax  categorical_crossentropy   \n",
       "20            32              2         softmax  categorical_crossentropy   \n",
       "21            64              6         softmax  categorical_crossentropy   \n",
       "22            64              4         softmax  categorical_crossentropy   \n",
       "23            32              4         softmax  categorical_crossentropy   \n",
       "24            32              6         softmax  categorical_crossentropy   \n",
       "25           128              1         softmax  categorical_crossentropy   \n",
       "26            32              3         softmax  categorical_crossentropy   \n",
       "27            16              5         softmax  categorical_crossentropy   \n",
       "28            64              0         softmax  categorical_crossentropy   \n",
       "29            32              0         softmax  categorical_crossentropy   \n",
       "30            16              4         softmax  categorical_crossentropy   \n",
       "31            16              3         softmax  categorical_crossentropy   \n",
       "32            32              5         softmax  categorical_crossentropy   \n",
       "33            64              2         softmax  categorical_crossentropy   \n",
       "34            32              2         softmax  categorical_crossentropy   \n",
       "35           128              3         softmax  categorical_crossentropy   \n",
       "36            16              0         softmax  categorical_crossentropy   \n",
       "37            16              5         softmax  categorical_crossentropy   \n",
       "38           128              6         softmax  categorical_crossentropy   \n",
       "39           128              3         softmax  categorical_crossentropy   \n",
       "40            64              3         softmax  categorical_crossentropy   \n",
       "41            32              0         softmax  categorical_crossentropy   \n",
       "42            64              2         softmax  categorical_crossentropy   \n",
       "43            64              5         softmax  categorical_crossentropy   \n",
       "44           128              6         softmax  categorical_crossentropy   \n",
       "45            16              2         softmax  categorical_crossentropy   \n",
       "46            32              1         softmax  categorical_crossentropy   \n",
       "47            64              4         softmax  categorical_crossentropy   \n",
       "48           128              5         softmax  categorical_crossentropy   \n",
       "49           128              1         softmax  categorical_crossentropy   \n",
       "\n",
       "       lr optimizer  \n",
       "0   0.010    Adamax  \n",
       "1   0.100      Adam  \n",
       "2   0.010   RMSprop  \n",
       "3   0.001   Adagrad  \n",
       "4   0.001      Adam  \n",
       "5   0.001   RMSprop  \n",
       "6   0.100       SGD  \n",
       "7   0.010      Adam  \n",
       "8   0.010     Nadam  \n",
       "9   0.010      Adam  \n",
       "10  0.010     Nadam  \n",
       "11  0.010  Adadelta  \n",
       "12  0.010  Adadelta  \n",
       "13  0.010  Adadelta  \n",
       "14  0.100     Nadam  \n",
       "15  0.001   RMSprop  \n",
       "16  0.001     Nadam  \n",
       "17  0.100     Nadam  \n",
       "18  0.010   RMSprop  \n",
       "19  0.010     Nadam  \n",
       "20  0.100       SGD  \n",
       "21  0.100       SGD  \n",
       "22  0.010  Adadelta  \n",
       "23  0.001    Adamax  \n",
       "24  0.001      Adam  \n",
       "25  0.001       SGD  \n",
       "26  0.010      Adam  \n",
       "27  0.100    Adamax  \n",
       "28  0.001   RMSprop  \n",
       "29  0.100     Nadam  \n",
       "30  0.010   Adagrad  \n",
       "31  0.010     Nadam  \n",
       "32  0.010    Adamax  \n",
       "33  0.010       SGD  \n",
       "34  0.010   RMSprop  \n",
       "35  0.100      Adam  \n",
       "36  0.001     Nadam  \n",
       "37  0.100  Adadelta  \n",
       "38  0.100   Adagrad  \n",
       "39  0.100  Adadelta  \n",
       "40  0.010  Adadelta  \n",
       "41  0.010    Adamax  \n",
       "42  0.010     Nadam  \n",
       "43  0.001       SGD  \n",
       "44  0.100      Adam  \n",
       "45  0.001    Adamax  \n",
       "46  0.100   Adagrad  \n",
       "47  0.100       SGD  \n",
       "48  0.010   RMSprop  \n",
       "49  0.001      Adam  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_f1score</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1score</th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>0.310333</td>\n",
       "      <td>0.875514</td>\n",
       "      <td>0.866490</td>\n",
       "      <td>0.395362</td>\n",
       "      <td>0.849647</td>\n",
       "      <td>0.837480</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>20</td>\n",
       "      <td>0.311218</td>\n",
       "      <td>0.872942</td>\n",
       "      <td>0.866684</td>\n",
       "      <td>0.331163</td>\n",
       "      <td>0.877645</td>\n",
       "      <td>0.865182</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>3.267373</td>\n",
       "      <td>0.168210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.264940</td>\n",
       "      <td>0.181878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>linear</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>20</td>\n",
       "      <td>0.611723</td>\n",
       "      <td>0.789609</td>\n",
       "      <td>0.775542</td>\n",
       "      <td>0.683515</td>\n",
       "      <td>0.774471</td>\n",
       "      <td>0.751053</td>\n",
       "      <td>elu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>20</td>\n",
       "      <td>0.834765</td>\n",
       "      <td>0.766975</td>\n",
       "      <td>0.662764</td>\n",
       "      <td>0.899848</td>\n",
       "      <td>0.738536</td>\n",
       "      <td>0.653120</td>\n",
       "      <td>selu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>3.271152</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.269151</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>softsign</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "      <td>1.703885</td>\n",
       "      <td>0.605967</td>\n",
       "      <td>0.102302</td>\n",
       "      <td>1.800833</td>\n",
       "      <td>0.536596</td>\n",
       "      <td>0.089517</td>\n",
       "      <td>exponential</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>30</td>\n",
       "      <td>0.445519</td>\n",
       "      <td>0.844136</td>\n",
       "      <td>0.819052</td>\n",
       "      <td>0.523338</td>\n",
       "      <td>0.816578</td>\n",
       "      <td>0.789595</td>\n",
       "      <td>softplus</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>0.293978</td>\n",
       "      <td>0.875514</td>\n",
       "      <td>0.862192</td>\n",
       "      <td>0.372052</td>\n",
       "      <td>0.857804</td>\n",
       "      <td>0.845098</td>\n",
       "      <td>softplus</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>1.005644</td>\n",
       "      <td>0.723765</td>\n",
       "      <td>0.563293</td>\n",
       "      <td>1.067845</td>\n",
       "      <td>0.703263</td>\n",
       "      <td>0.559790</td>\n",
       "      <td>tanh</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>0.284970</td>\n",
       "      <td>0.881173</td>\n",
       "      <td>0.874117</td>\n",
       "      <td>0.331083</td>\n",
       "      <td>0.864859</td>\n",
       "      <td>0.861411</td>\n",
       "      <td>elu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>20</td>\n",
       "      <td>0.214697</td>\n",
       "      <td>0.905864</td>\n",
       "      <td>0.903338</td>\n",
       "      <td>0.221494</td>\n",
       "      <td>0.906305</td>\n",
       "      <td>0.905259</td>\n",
       "      <td>linear</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>0.486140</td>\n",
       "      <td>0.837963</td>\n",
       "      <td>0.807953</td>\n",
       "      <td>0.528769</td>\n",
       "      <td>0.826499</td>\n",
       "      <td>0.800539</td>\n",
       "      <td>softmax</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>0.641587</td>\n",
       "      <td>0.811214</td>\n",
       "      <td>0.736287</td>\n",
       "      <td>0.692329</td>\n",
       "      <td>0.792769</td>\n",
       "      <td>0.729503</td>\n",
       "      <td>exponential</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.396851</td>\n",
       "      <td>0.846193</td>\n",
       "      <td>0.833050</td>\n",
       "      <td>0.416782</td>\n",
       "      <td>0.842813</td>\n",
       "      <td>0.830685</td>\n",
       "      <td>selu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>3.229803</td>\n",
       "      <td>0.196502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.230191</td>\n",
       "      <td>0.219136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>softplus</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>30</td>\n",
       "      <td>0.270044</td>\n",
       "      <td>0.889918</td>\n",
       "      <td>0.880114</td>\n",
       "      <td>0.320305</td>\n",
       "      <td>0.873898</td>\n",
       "      <td>0.867641</td>\n",
       "      <td>softplus</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>20</td>\n",
       "      <td>0.170381</td>\n",
       "      <td>0.914095</td>\n",
       "      <td>0.914380</td>\n",
       "      <td>0.168276</td>\n",
       "      <td>0.917989</td>\n",
       "      <td>0.916912</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>3.015559</td>\n",
       "      <td>0.297325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.045150</td>\n",
       "      <td>0.318342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>selu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>0.713233</td>\n",
       "      <td>0.779321</td>\n",
       "      <td>0.724849</td>\n",
       "      <td>0.776726</td>\n",
       "      <td>0.766534</td>\n",
       "      <td>0.706460</td>\n",
       "      <td>linear</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>0.263235</td>\n",
       "      <td>0.890947</td>\n",
       "      <td>0.886043</td>\n",
       "      <td>0.270672</td>\n",
       "      <td>0.895503</td>\n",
       "      <td>0.887516</td>\n",
       "      <td>elu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>30</td>\n",
       "      <td>0.575054</td>\n",
       "      <td>0.820473</td>\n",
       "      <td>0.780359</td>\n",
       "      <td>0.640328</td>\n",
       "      <td>0.788801</td>\n",
       "      <td>0.755062</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309363</td>\n",
       "      <td>0.876543</td>\n",
       "      <td>0.862575</td>\n",
       "      <td>0.325407</td>\n",
       "      <td>0.880511</td>\n",
       "      <td>0.867509</td>\n",
       "      <td>softplus</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>30</td>\n",
       "      <td>0.502778</td>\n",
       "      <td>0.831276</td>\n",
       "      <td>0.799756</td>\n",
       "      <td>0.529614</td>\n",
       "      <td>0.823633</td>\n",
       "      <td>0.783145</td>\n",
       "      <td>softmax</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>10</td>\n",
       "      <td>0.630345</td>\n",
       "      <td>0.807099</td>\n",
       "      <td>0.754378</td>\n",
       "      <td>0.674513</td>\n",
       "      <td>0.790785</td>\n",
       "      <td>0.741984</td>\n",
       "      <td>softmax</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>30</td>\n",
       "      <td>0.643627</td>\n",
       "      <td>0.801955</td>\n",
       "      <td>0.754855</td>\n",
       "      <td>0.784778</td>\n",
       "      <td>0.746693</td>\n",
       "      <td>0.704239</td>\n",
       "      <td>softmax</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>1.229270</td>\n",
       "      <td>0.683642</td>\n",
       "      <td>0.498649</td>\n",
       "      <td>1.356261</td>\n",
       "      <td>0.625882</td>\n",
       "      <td>0.397776</td>\n",
       "      <td>softplus</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>30</td>\n",
       "      <td>0.182849</td>\n",
       "      <td>0.907922</td>\n",
       "      <td>0.906893</td>\n",
       "      <td>0.196754</td>\n",
       "      <td>0.906305</td>\n",
       "      <td>0.904933</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>20</td>\n",
       "      <td>2.185394</td>\n",
       "      <td>0.498971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.241196</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>softmax</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>10</td>\n",
       "      <td>0.625896</td>\n",
       "      <td>0.798354</td>\n",
       "      <td>0.761771</td>\n",
       "      <td>0.671816</td>\n",
       "      <td>0.793210</td>\n",
       "      <td>0.751925</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>0.799383</td>\n",
       "      <td>0.767706</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.758377</td>\n",
       "      <td>0.709478</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>20</td>\n",
       "      <td>1.346598</td>\n",
       "      <td>0.656893</td>\n",
       "      <td>0.334207</td>\n",
       "      <td>1.369712</td>\n",
       "      <td>0.663801</td>\n",
       "      <td>0.350397</td>\n",
       "      <td>elu</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>20</td>\n",
       "      <td>0.250307</td>\n",
       "      <td>0.894033</td>\n",
       "      <td>0.889630</td>\n",
       "      <td>0.322836</td>\n",
       "      <td>0.867945</td>\n",
       "      <td>0.861647</td>\n",
       "      <td>elu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951691</td>\n",
       "      <td>0.741255</td>\n",
       "      <td>0.567352</td>\n",
       "      <td>1.057486</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.566913</td>\n",
       "      <td>softsign</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>RMSprop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>10</td>\n",
       "      <td>3.207749</td>\n",
       "      <td>0.252572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.210027</td>\n",
       "      <td>0.260802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>30</td>\n",
       "      <td>0.320615</td>\n",
       "      <td>0.869342</td>\n",
       "      <td>0.864487</td>\n",
       "      <td>0.379452</td>\n",
       "      <td>0.857363</td>\n",
       "      <td>0.848625</td>\n",
       "      <td>elu</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>30</td>\n",
       "      <td>0.384420</td>\n",
       "      <td>0.852366</td>\n",
       "      <td>0.841676</td>\n",
       "      <td>0.453485</td>\n",
       "      <td>0.826058</td>\n",
       "      <td>0.814342</td>\n",
       "      <td>tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>20</td>\n",
       "      <td>0.440706</td>\n",
       "      <td>0.841564</td>\n",
       "      <td>0.830087</td>\n",
       "      <td>0.462946</td>\n",
       "      <td>0.838404</td>\n",
       "      <td>0.819075</td>\n",
       "      <td>tanh</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>20</td>\n",
       "      <td>0.353083</td>\n",
       "      <td>0.866255</td>\n",
       "      <td>0.847288</td>\n",
       "      <td>0.486013</td>\n",
       "      <td>0.823192</td>\n",
       "      <td>0.807505</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>30</td>\n",
       "      <td>0.630630</td>\n",
       "      <td>0.809156</td>\n",
       "      <td>0.735299</td>\n",
       "      <td>0.655798</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.737216</td>\n",
       "      <td>elu</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adagrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>0.184834</td>\n",
       "      <td>0.897119</td>\n",
       "      <td>0.893676</td>\n",
       "      <td>0.180885</td>\n",
       "      <td>0.916226</td>\n",
       "      <td>0.916428</td>\n",
       "      <td>softmax</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>30</td>\n",
       "      <td>0.514068</td>\n",
       "      <td>0.828704</td>\n",
       "      <td>0.812064</td>\n",
       "      <td>0.601850</td>\n",
       "      <td>0.797839</td>\n",
       "      <td>0.770920</td>\n",
       "      <td>softsign</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>20</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.871914</td>\n",
       "      <td>0.862296</td>\n",
       "      <td>0.363141</td>\n",
       "      <td>0.864418</td>\n",
       "      <td>0.850755</td>\n",
       "      <td>exponential</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>0.224744</td>\n",
       "      <td>0.899177</td>\n",
       "      <td>0.897382</td>\n",
       "      <td>0.259780</td>\n",
       "      <td>0.887787</td>\n",
       "      <td>0.885388</td>\n",
       "      <td>softsign</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Nadam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>20</td>\n",
       "      <td>0.547473</td>\n",
       "      <td>0.820473</td>\n",
       "      <td>0.792297</td>\n",
       "      <td>0.575413</td>\n",
       "      <td>0.809744</td>\n",
       "      <td>0.780643</td>\n",
       "      <td>exponential</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>30</td>\n",
       "      <td>0.433377</td>\n",
       "      <td>0.843107</td>\n",
       "      <td>0.823682</td>\n",
       "      <td>0.457308</td>\n",
       "      <td>0.834656</td>\n",
       "      <td>0.822573</td>\n",
       "      <td>softplus</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.001</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>20</td>\n",
       "      <td>0.317020</td>\n",
       "      <td>0.867798</td>\n",
       "      <td>0.866512</td>\n",
       "      <td>0.333124</td>\n",
       "      <td>0.876323</td>\n",
       "      <td>0.867286</td>\n",
       "      <td>relu</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>10</td>\n",
       "      <td>3.213871</td>\n",
       "      <td>0.253601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.217178</td>\n",
       "      <td>0.254850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>30</td>\n",
       "      <td>0.460231</td>\n",
       "      <td>0.835905</td>\n",
       "      <td>0.810857</td>\n",
       "      <td>0.545109</td>\n",
       "      <td>0.811287</td>\n",
       "      <td>0.791109</td>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Adadelta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "      <td>1.307673</td>\n",
       "      <td>0.656893</td>\n",
       "      <td>0.282641</td>\n",
       "      <td>1.441581</td>\n",
       "      <td>0.618386</td>\n",
       "      <td>0.243559</td>\n",
       "      <td>softmax</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>softmax</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adamax</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  round_epochs  val_loss   val_acc  val_f1score      loss  \\\n",
       "50          50            10  0.310333  0.875514     0.866490  0.395362   \n",
       "51          51            20  0.311218  0.872942     0.866684  0.331163   \n",
       "52          52            20  3.267373  0.168210     0.000000  3.264940   \n",
       "53          53            20  0.611723  0.789609     0.775542  0.683515   \n",
       "54          54            20  0.834765  0.766975     0.662764  0.899848   \n",
       "55          55            10  3.271152  0.166667     0.000000  3.269151   \n",
       "56          56            10  1.703885  0.605967     0.102302  1.800833   \n",
       "57          57            30  0.445519  0.844136     0.819052  0.523338   \n",
       "58          58            30  0.293978  0.875514     0.862192  0.372052   \n",
       "59          59            10  1.005644  0.723765     0.563293  1.067845   \n",
       "60          60            20  0.284970  0.881173     0.874117  0.331083   \n",
       "61          61            20  0.214697  0.905864     0.903338  0.221494   \n",
       "62          62            10  0.486140  0.837963     0.807953  0.528769   \n",
       "63          63            10  0.641587  0.811214     0.736287  0.692329   \n",
       "64          64            10  0.396851  0.846193     0.833050  0.416782   \n",
       "65          65            10  3.229803  0.196502     0.000000  3.230191   \n",
       "66          66            30  0.270044  0.889918     0.880114  0.320305   \n",
       "67          67            20  0.170381  0.914095     0.914380  0.168276   \n",
       "68          68            10  3.015559  0.297325     0.000000  3.045150   \n",
       "69          69            10  0.713233  0.779321     0.724849  0.776726   \n",
       "70          70            30  0.263235  0.890947     0.886043  0.270672   \n",
       "71          71            30  0.575054  0.820473     0.780359  0.640328   \n",
       "72          72            30  0.309363  0.876543     0.862575  0.325407   \n",
       "73          73            30  0.502778  0.831276     0.799756  0.529614   \n",
       "74          74            10  0.630345  0.807099     0.754378  0.674513   \n",
       "75          75            30  0.643627  0.801955     0.754855  0.784778   \n",
       "76          76            10  1.229270  0.683642     0.498649  1.356261   \n",
       "77          77            30  0.182849  0.907922     0.906893  0.196754   \n",
       "78          78            20  2.185394  0.498971     0.000000  2.241196   \n",
       "79          79            10  0.625896  0.798354     0.761771  0.671816   \n",
       "80          80            30  0.605513  0.799383     0.767706  0.748000   \n",
       "81          81            20  1.346598  0.656893     0.334207  1.369712   \n",
       "82          82            20  0.250307  0.894033     0.889630  0.322836   \n",
       "83          83            10  0.951691  0.741255     0.567352  1.057486   \n",
       "84          84            10  3.207749  0.252572     0.000000  3.210027   \n",
       "85          85            30  0.320615  0.869342     0.864487  0.379452   \n",
       "86          86            30  0.384420  0.852366     0.841676  0.453485   \n",
       "87          87            20  0.440706  0.841564     0.830087  0.462946   \n",
       "88          88            20  0.353083  0.866255     0.847288  0.486013   \n",
       "89          89            30  0.630630  0.809156     0.735299  0.655798   \n",
       "90          90            30  0.184834  0.897119     0.893676  0.180885   \n",
       "91          91            30  0.514068  0.828704     0.812064  0.601850   \n",
       "92          92            20  0.309060  0.871914     0.862296  0.363141   \n",
       "93          93            30  0.224744  0.899177     0.897382  0.259780   \n",
       "94          94            20  0.547473  0.820473     0.792297  0.575413   \n",
       "95          95            30  0.433377  0.843107     0.823682  0.457308   \n",
       "96          96            20  0.317020  0.867798     0.866512  0.333124   \n",
       "97          97            10  3.213871  0.253601     0.000000  3.217178   \n",
       "98          98            30  0.460231  0.835905     0.810857  0.545109   \n",
       "99          99            10  1.307673  0.656893     0.282641  1.441581   \n",
       "\n",
       "         acc   f1score    activation  batch_size  dropout  epochs  \\\n",
       "50  0.849647  0.837480   exponential           5      0.2      10   \n",
       "51  0.877645  0.865182   exponential          50      0.0      20   \n",
       "52  0.181878  0.000000        linear          50      0.0      20   \n",
       "53  0.774471  0.751053           elu          50      0.2      20   \n",
       "54  0.738536  0.653120          selu          50      0.2      20   \n",
       "55  0.188933  0.000000      softsign          25      0.0      10   \n",
       "56  0.536596  0.089517   exponential          10      0.2      10   \n",
       "57  0.816578  0.789595      softplus           5      0.2      30   \n",
       "58  0.857804  0.845098      softplus          10      0.2      30   \n",
       "59  0.703263  0.559790          tanh          25      0.2      10   \n",
       "60  0.864859  0.861411           elu          50      0.2      20   \n",
       "61  0.906305  0.905259        linear          25      0.0      20   \n",
       "62  0.826499  0.800539       softmax          25      0.0      10   \n",
       "63  0.792769  0.729503   exponential          25      0.0      10   \n",
       "64  0.842813  0.830685          selu           5      0.0      10   \n",
       "65  0.219136  0.000000      softplus          10      0.0      10   \n",
       "66  0.873898  0.867641      softplus          50      0.2      30   \n",
       "67  0.917989  0.916912          tanh           5      0.0      20   \n",
       "68  0.318342  0.000000          selu           5      0.0      10   \n",
       "69  0.766534  0.706460        linear          25      0.2      10   \n",
       "70  0.895503  0.887516           elu           5      0.0      30   \n",
       "71  0.788801  0.755062       sigmoid          50      0.2      30   \n",
       "72  0.880511  0.867509      softplus          50      0.0      30   \n",
       "73  0.823633  0.783145       softmax           5      0.0      30   \n",
       "74  0.790785  0.741984       softmax          50      0.0      10   \n",
       "75  0.746693  0.704239       softmax          25      0.2      30   \n",
       "76  0.625882  0.397776      softplus          50      0.2      10   \n",
       "77  0.906305  0.904933       sigmoid           5      0.2      30   \n",
       "78  0.462963  0.000000       softmax           5      0.2      20   \n",
       "79  0.793210  0.751925       sigmoid          10      0.0      10   \n",
       "80  0.758377  0.709478       sigmoid          10      0.2      30   \n",
       "81  0.663801  0.350397           elu          25      0.0      20   \n",
       "82  0.867945  0.861647           elu          10      0.2      20   \n",
       "83  0.703704  0.566913      softsign          50      0.2      10   \n",
       "84  0.260802  0.000000   exponential          10      0.2      10   \n",
       "85  0.857363  0.848625           elu          25      0.2      30   \n",
       "86  0.826058  0.814342          tanh           5      0.2      30   \n",
       "87  0.838404  0.819075          tanh          25      0.0      20   \n",
       "88  0.823192  0.807505       sigmoid          10      0.2      20   \n",
       "89  0.803571  0.737216           elu           5      0.0      30   \n",
       "90  0.916226  0.916428       softmax           5      0.0      30   \n",
       "91  0.797839  0.770920      softsign          50      0.2      30   \n",
       "92  0.864418  0.850755   exponential          10      0.2      20   \n",
       "93  0.887787  0.885388      softsign          50      0.2      30   \n",
       "94  0.809744  0.780643   exponential          10      0.0      20   \n",
       "95  0.834656  0.822573      softplus          10      0.0      30   \n",
       "96  0.876323  0.867286          relu          25      0.0      20   \n",
       "97  0.254850  0.000000          relu          10      0.2      10   \n",
       "98  0.811287  0.791109  hard_sigmoid          10      0.2      30   \n",
       "99  0.618386  0.243559       softmax          10      0.2      10   \n",
       "\n",
       "    first_neuron  hidden_layers last_activation                    losses  \\\n",
       "50            32              1         softmax  categorical_crossentropy   \n",
       "51            64              0         softmax  categorical_crossentropy   \n",
       "52            64              3         softmax  categorical_crossentropy   \n",
       "53            64              2         softmax  categorical_crossentropy   \n",
       "54            64              0         softmax  categorical_crossentropy   \n",
       "55            64              2         softmax  categorical_crossentropy   \n",
       "56            16              5         softmax  categorical_crossentropy   \n",
       "57            32              4         softmax  categorical_crossentropy   \n",
       "58            32              1         softmax  categorical_crossentropy   \n",
       "59            64              6         softmax  categorical_crossentropy   \n",
       "60           128              1         softmax  categorical_crossentropy   \n",
       "61           128              2         softmax  categorical_crossentropy   \n",
       "62            32              2         softmax  categorical_crossentropy   \n",
       "63            16              6         softmax  categorical_crossentropy   \n",
       "64           128              1         softmax  categorical_crossentropy   \n",
       "65            32              2         softmax  categorical_crossentropy   \n",
       "66            64              1         softmax  categorical_crossentropy   \n",
       "67            64              4         softmax  categorical_crossentropy   \n",
       "68            32              5         softmax  categorical_crossentropy   \n",
       "69           128              0         softmax  categorical_crossentropy   \n",
       "70           128              3         softmax  categorical_crossentropy   \n",
       "71            64              5         softmax  categorical_crossentropy   \n",
       "72           128              4         softmax  categorical_crossentropy   \n",
       "73            16              0         softmax  categorical_crossentropy   \n",
       "74            32              0         softmax  categorical_crossentropy   \n",
       "75            16              2         softmax  categorical_crossentropy   \n",
       "76            16              6         softmax  categorical_crossentropy   \n",
       "77           128              4         softmax  categorical_crossentropy   \n",
       "78            64              0         softmax  categorical_crossentropy   \n",
       "79            32              6         softmax  categorical_crossentropy   \n",
       "80            16              1         softmax  categorical_crossentropy   \n",
       "81            16              6         softmax  categorical_crossentropy   \n",
       "82            32              6         softmax  categorical_crossentropy   \n",
       "83            64              5         softmax  categorical_crossentropy   \n",
       "84            64              3         softmax  categorical_crossentropy   \n",
       "85            64              6         softmax  categorical_crossentropy   \n",
       "86            64              3         softmax  categorical_crossentropy   \n",
       "87           128              1         softmax  categorical_crossentropy   \n",
       "88            16              0         softmax  categorical_crossentropy   \n",
       "89            64              0         softmax  categorical_crossentropy   \n",
       "90            16              3         softmax  categorical_crossentropy   \n",
       "91            32              1         softmax  categorical_crossentropy   \n",
       "92            64              4         softmax  categorical_crossentropy   \n",
       "93           128              2         softmax  categorical_crossentropy   \n",
       "94            32              0         softmax  categorical_crossentropy   \n",
       "95            32              5         softmax  categorical_crossentropy   \n",
       "96           128              0         softmax  categorical_crossentropy   \n",
       "97            64              1         softmax  categorical_crossentropy   \n",
       "98            32              3         softmax  categorical_crossentropy   \n",
       "99            16              3         softmax  categorical_crossentropy   \n",
       "\n",
       "       lr optimizer  \n",
       "50  0.010     Nadam  \n",
       "51  0.010     Nadam  \n",
       "52  0.100       SGD  \n",
       "53  0.001  Adadelta  \n",
       "54  0.001   Adagrad  \n",
       "55  0.001       SGD  \n",
       "56  0.010   Adagrad  \n",
       "57  0.001    Adamax  \n",
       "58  0.100      Adam  \n",
       "59  0.001   Adagrad  \n",
       "60  0.100     Nadam  \n",
       "61  0.010     Nadam  \n",
       "62  0.010     Nadam  \n",
       "63  0.100     Nadam  \n",
       "64  0.010   RMSprop  \n",
       "65  0.010       SGD  \n",
       "66  0.001     Nadam  \n",
       "67  0.001     Nadam  \n",
       "68  0.100       SGD  \n",
       "69  0.010    Adamax  \n",
       "70  0.001    Adamax  \n",
       "71  0.001    Adamax  \n",
       "72  0.100      Adam  \n",
       "73  0.010    Adamax  \n",
       "74  0.100     Nadam  \n",
       "75  0.010  Adadelta  \n",
       "76  0.010  Adadelta  \n",
       "77  0.001      Adam  \n",
       "78  0.100       SGD  \n",
       "79  0.001   RMSprop  \n",
       "80  0.010  Adadelta  \n",
       "81  0.010   Adagrad  \n",
       "82  0.001     Nadam  \n",
       "83  0.100   RMSprop  \n",
       "84  0.100       SGD  \n",
       "85  0.001      Adam  \n",
       "86  0.001  Adadelta  \n",
       "87  0.001  Adadelta  \n",
       "88  0.010     Nadam  \n",
       "89  0.010   Adagrad  \n",
       "90  0.001     Nadam  \n",
       "91  0.100      Adam  \n",
       "92  0.010      Adam  \n",
       "93  0.100     Nadam  \n",
       "94  0.010    Adamax  \n",
       "95  0.001    Adamax  \n",
       "96  0.010      Adam  \n",
       "97  0.100       SGD  \n",
       "98  0.100  Adadelta  \n",
       "99  0.010    Adamax  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
